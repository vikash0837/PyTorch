{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch tutorial-2(AUTOGRAD: AUTOMATIC DIFFERENTIATION).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikash0837/PyTorch/blob/master/Pytorch_tutorial_2(AUTOGRAD_AUTOMATIC_DIFFERENTIATION).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaU9diFqA5pl",
        "colab_type": "text"
      },
      "source": [
        "# **Learning Pytorch Tutorial basics**\n",
        "[Source](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXGhMBFrBKgE",
        "colab_type": "text"
      },
      "source": [
        "# **AUTOGRAD: AUTOMATIC DIFFERENTIATION**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   `autograd` is Center to all neural network\n",
        "*   `autograd` provides automatic differentiation for all operations on Tensors\n",
        "*   It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JOOwNPICaJk",
        "colab_type": "text"
      },
      "source": [
        "## **Tensor**\n",
        "\n",
        "\n",
        "*   `torch.Tensor` is the central class of the package\n",
        "*   If you set its attribute `.requires_grad` as `True`, it starts to track all operations on it\n",
        "* When you finish your computation you can call `.backward()` and have all the gradients computed automatically\n",
        "* The gradient for this tensor will be accumulated into `.grad` attribute.\n",
        "* To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n",
        "\n",
        "* To prevent tracking history (and using memory), you can also wrap the code block in with `torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n",
        "\n",
        "* There’s one more class which is very important for autograd implementation `- a Function`.\n",
        "* Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation\n",
        "* Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their `grad_fn` is `None`).\n",
        "* If you want to compute the derivatives, you can call `.backward()` on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3XSRNopA3DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNfQHsqnEvsS",
        "colab_type": "code",
        "outputId": "7a9cc969-52b5-457e-ff99-05af02842bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create a tensor and set requires_grad=True to track computation with it\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIoIyCTyE1Se",
        "colab_type": "code",
        "outputId": "e7e07bf1-1376-4688-f3a4-99f798e8c499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Do a tensor operation:\n",
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyRi4i9LE9aP",
        "colab_type": "code",
        "outputId": "b1b13642-9d73-4737-ed60-2a31c4f6e12b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# y was created as a result of an operation, so it has a grad_fn.\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7f4eadf80470>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGOcy6mKFMNO",
        "colab_type": "code",
        "outputId": "fd8172cb-cddc-4b72-d225-9ee8e9a1fbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# more operation on y\n",
        "z = y*y*3\n",
        "out = z.mean()\n",
        "print(z, out)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceDDhKa3GqLc",
        "colab_type": "text"
      },
      "source": [
        "Note: `.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag in-place. The input flag defaults to False if not given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN7vLH95FvOu",
        "colab_type": "code",
        "outputId": "80e76358-e1e1-49b9-c483-ba8518dca077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7f4eadf84358>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kchZwPXgIM-C",
        "colab_type": "code",
        "outputId": "545b693f-592f-4af8-8b40-c26416a38117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.7722, 46.1572],\n",
            "        [-2.5183, -7.0657]], requires_grad=True)\n",
            "tensor(2189.8940, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uHgqtz7HZsA",
        "colab_type": "text"
      },
      "source": [
        "# Gradients\n",
        "\n",
        "\n",
        "> Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to out.backward(torch.tensor(1.)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFzw3KbG6nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n76K487Iac9",
        "colab_type": "code",
        "outputId": "f2983ee0-af34-4f84-fc24-06f1592795d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Print gradients d(out)/dx\n",
        "print(x.grad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2LIP9TpIeWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f34fbe89-d11e-4724-ff60-5f9cd7a84e6e"
      },
      "source": [
        "# et’s take a look at an example of vector-Jacobian product:\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x * 2\n",
        "print(y)\n",
        "i=0\n",
        "#print(\"iteration:\",i)\n",
        "while y.data.norm() < 10:\n",
        "    print(\"iteration:\",i)\n",
        "    y = y * 2\n",
        "    i+=1\n",
        "    \n",
        "\n",
        "print(y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.7681,  2.7217,  0.7993], requires_grad=True)\n",
            "tensor([-1.5363,  5.4435,  1.5987], grad_fn=<MulBackward0>)\n",
            "iteration: 0\n",
            "tensor([-3.0725, 10.8870,  3.1974], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgabtTNrgQp4",
        "colab_type": "text"
      },
      "source": [
        "## Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh7aQMXCd31D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a43ed03-7950-4ce6-a63c-49c796559c89"
      },
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.0000e-01, 4.0000e+00, 4.0000e-04])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhLDadmqe6dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6fb1089e-afd0-4534-ed80-6de2d3a35c3a"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSJPKf3uhCkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73eff105-22aa-4cf4-a172-4d780f561911"
      },
      "source": [
        "# Or by using .detach() to get a new Tensor with the same content but that does not require gradients:\n",
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG3XjT3mhf6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}